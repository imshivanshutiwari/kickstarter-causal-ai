{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 04 - Validation\n",
                "\n",
                "This notebook validates counterfactual predictions using:\n",
                "1. Temporal validation with matched campaign pairs\n",
                "2. Placebo tests\n",
                "3. Manski bounds analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "sys.path.insert(0, '../src')\n",
                "\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "from causal_inference import CounterfactualPredictor\n",
                "from validation import (\n",
                "    TemporalValidator,\n",
                "    PlaceboTester,\n",
                "    ManskiBoundsAnalyzer,\n",
                "    run_full_validation\n",
                ")\n",
                "\n",
                "sns.set_theme(style='darkgrid')\n",
                "plt.rcParams['figure.figsize'] = (12, 6)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load data and models\n",
                "df = pd.read_csv('../data/raw/kickstarter_raw_data.csv')\n",
                "\n",
                "predictor = CounterfactualPredictor()\n",
                "predictor.load_models('../data/processed/causal_models.pkl')\n",
                "\n",
                "print(f\"Loaded {len(df)} campaigns and trained models\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Temporal Validation\n",
                "\n",
                "Find pairs of similar campaigns with different strategies and validate predictions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "temporal = TemporalValidator()\n",
                "pairs = temporal.identify_strategy_changes(df)\n",
                "print(f\"Found {len(pairs)} matched campaign pairs\")\n",
                "pairs.head(10)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize price changes and outcome changes\n",
                "fig, ax = plt.subplots(figsize=(10, 6))\n",
                "\n",
                "ax.scatter(pairs['price_change_pct'], pairs['outcome_change'], alpha=0.5)\n",
                "ax.axhline(y=0, color='red', linestyle='--')\n",
                "ax.axvline(x=0, color='red', linestyle='--')\n",
                "ax.set_xlabel('Price Change (%)')\n",
                "ax.set_ylabel('Outcome Change (Funding Ratio)')\n",
                "ax.set_title('Price Changes vs Outcome Changes in Matched Pairs')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Placebo Test\n",
                "\n",
                "For campaigns that didn't change, model should predict no effect."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "placebo = PlaceboTester()\n",
                "placebo_results = placebo.run_placebo_test(df, predictor, n_samples=100)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Placebo Test Results:\")\n",
                "print(f\"  Mean Placebo Effect: {placebo_results['mean_placebo_effect']:.4f}\")\n",
                "print(f\"  Std Placebo Effect: {placebo_results['std_placebo_effect']:.4f}\")\n",
                "print(f\"  95th Percentile: {placebo_results['percentile_95']:.4f}\")\n",
                "print(f\"  Test Passed: {placebo_results['passed']}\")\n",
                "\n",
                "if placebo_results['passed']:\n",
                "    print(\"\\n✓ Good! Model doesn't find spurious effects where none exist.\")\n",
                "else:\n",
                "    print(\"\\n✗ Warning: Model may be capturing noise rather than true causal effects.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Manski Bounds\n",
                "\n",
                "Establish plausible ranges for counterfactual estimates."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "bounds = ManskiBoundsAnalyzer()\n",
                "bounds_results = bounds.analyze_dataset_bounds(df, predictor, price_change_pct=-20)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Manski Bounds Results:\")\n",
                "print(f\"  Samples Analyzed: {bounds_results['n_samples']}\")\n",
                "print(f\"  Avg Point Estimate: {bounds_results['avg_point_estimate']:.4f}\")\n",
                "print(f\"  Avg Interval Width: {bounds_results['avg_interval_width']:.4f}\")\n",
                "print(f\"  Estimates Within Bounds: {bounds_results['estimates_within_bounds']*100:.1f}%\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize bounds for sample campaigns\n",
                "if 'bounds_sample' in bounds_results:\n",
                "    sample_bounds = bounds_results['bounds_sample']\n",
                "    \n",
                "    fig, ax = plt.subplots(figsize=(10, 6))\n",
                "    \n",
                "    x = range(len(sample_bounds))\n",
                "    point_estimates = [b['point_estimate'] for b in sample_bounds]\n",
                "    lower = [b['manski_lower'] for b in sample_bounds]\n",
                "    upper = [b['manski_upper'] for b in sample_bounds]\n",
                "    \n",
                "    ax.scatter(x, point_estimates, color='red', s=100, zorder=5, label='Point Estimate')\n",
                "    ax.vlines(x, lower, upper, color='blue', linewidth=2, label='Manski Bounds')\n",
                "    \n",
                "    ax.set_xlabel('Campaign')\n",
                "    ax.set_ylabel('Counterfactual Funding Ratio')\n",
                "    ax.set_title('Point Estimates with Manski Bounds')\n",
                "    ax.legend()\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Full Validation Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run full validation suite\n",
                "all_results = run_full_validation(df, predictor)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Summary table\n",
                "summary = {\n",
                "    'Metric': ['Temporal MAE', 'Placebo Mean Effect', 'Placebo Test Passed', 'Bounds Within Range'],\n",
                "    'Value': [\n",
                "        all_results.get('temporal', {}).get('mae', 'N/A'),\n",
                "        all_results.get('placebo', {}).get('mean_placebo_effect', 'N/A'),\n",
                "        all_results.get('placebo', {}).get('passed', 'N/A'),\n",
                "        f\"{all_results.get('bounds', {}).get('estimates_within_bounds', 0)*100:.1f}%\"\n",
                "    ]\n",
                "}\n",
                "\n",
                "summary_df = pd.DataFrame(summary)\n",
                "print(\"\\nValidation Summary:\")\n",
                "print(summary_df.to_string(index=False))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}