{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 05 - Counterfactual Validation\n",
                "\n",
                "This notebook validates our causal models using:\n",
                "1. **Temporal Validation**: Test predictions on campaigns that changed strategy\n",
                "2. **Placebo Test**: Model should find no effect when there is none\n",
                "3. **Manski Bounds**: Check if estimates are within theoretical bounds\n",
                "4. **Temporal Cross-Validation**: Test stability of causal effects over time"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.linear_model import LinearRegression\n",
                "from sklearn.ensemble import GradientBoostingRegressor\n",
                "from sklearn.metrics import r2_score, mean_absolute_error\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "import pickle\n",
                "import warnings\n",
                "\n",
                "warnings.filterwarnings('ignore')\n",
                "sns.set_theme(style='darkgrid')\n",
                "plt.rcParams['figure.figsize'] = (12, 6)\n",
                "\n",
                "%matplotlib inline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load data and models\n",
                "df = pd.read_csv('../data/processed/kickstarter_causal_features.csv')\n",
                "df['launch_date'] = pd.to_datetime(df['launch_date'])\n",
                "\n",
                "# Load trained models\n",
                "with open('../src/models/causal_models.pkl', 'rb') as f:\n",
                "    models = pickle.load(f)\n",
                "\n",
                "print(f\"Loaded {len(df)} campaigns\")\n",
                "print(f\"Models available: {list(models.keys())}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare features\n",
                "duration_col = 'campaign_duration_days' if 'campaign_duration_days' in df.columns else 'duration_days'\n",
                "feature_cols = ['avg_reward_price', 'goal_ambition', duration_col, 'trend_index', 'concurrent_campaigns']\n",
                "\n",
                "# Store validation results\n",
                "validation_results = {}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Validation 1: Temporal Validation\n",
                "\n",
                "Identify campaigns with price variation (proxy for strategy experimentation) and validate predictions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Identify campaigns with high price spread (proxy for price experimentation)\n",
                "df['price_variation'] = df['price_spread'] / df['avg_reward_price'].replace(0, 1)\n",
                "\n",
                "# Select top 10 campaigns with most price variation as \"strategy changers\"\n",
                "validation_campaigns = df.nlargest(10, 'price_variation').copy()\n",
                "\n",
                "print(f\"Selected {len(validation_campaigns)} campaigns for temporal validation\")\n",
                "print(f\"Price variation range: {validation_campaigns['price_variation'].min():.2f} - {validation_campaigns['price_variation'].max():.2f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def validate_campaign(campaign, model, scaler, feature_cols):\n",
                "    \"\"\"\n",
                "    Validate counterfactual prediction for a campaign.\n",
                "    \n",
                "    Simulates: \"What if they used their min price instead of avg price?\"\n",
                "    \"\"\"\n",
                "    # Original features\n",
                "    original_features = campaign[feature_cols].values.reshape(1, -1)\n",
                "    \n",
                "    # Counterfactual: use min price instead of avg\n",
                "    cf_features = original_features.copy()\n",
                "    price_idx = feature_cols.index('avg_reward_price')\n",
                "    cf_features[0, price_idx] = campaign['min_reward_price']\n",
                "    \n",
                "    # Scale and predict\n",
                "    original_scaled = scaler.transform(original_features)\n",
                "    cf_scaled = scaler.transform(cf_features)\n",
                "    \n",
                "    original_pred = model.predict(original_scaled)[0]\n",
                "    cf_pred = model.predict(cf_scaled)[0]\n",
                "    \n",
                "    actual = campaign['funding_ratio']\n",
                "    error = abs(original_pred - actual)\n",
                "    predicted_effect = cf_pred - original_pred\n",
                "    \n",
                "    return {\n",
                "        'campaign_id': campaign.name,\n",
                "        'actual': actual,\n",
                "        'predicted': original_pred,\n",
                "        'error': error,\n",
                "        'cf_predicted': cf_pred,\n",
                "        'predicted_effect': predicted_effect,\n",
                "        'price_change': campaign['min_reward_price'] - campaign['avg_reward_price']\n",
                "    }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run validation\n",
                "scaler = models['scaler']\n",
                "ols_model = models['ols']\n",
                "\n",
                "temporal_results = []\n",
                "for idx, row in validation_campaigns.iterrows():\n",
                "    result = validate_campaign(row, ols_model, scaler, feature_cols)\n",
                "    temporal_results.append(result)\n",
                "\n",
                "temporal_df = pd.DataFrame(temporal_results)\n",
                "\n",
                "print(\"\\nTEMPORAL VALIDATION RESULTS\")\n",
                "print(\"=\"*50)\n",
                "print(f\"Mean Absolute Error: {temporal_df['error'].mean():.4f}\")\n",
                "print(f\"Median Absolute Error: {temporal_df['error'].median():.4f}\")\n",
                "print(f\"90th percentile error: {temporal_df['error'].quantile(0.9):.4f}\")\n",
                "\n",
                "validation_results['temporal'] = {\n",
                "    'mae': temporal_df['error'].mean(),\n",
                "    'median_ae': temporal_df['error'].median(),\n",
                "    'p90_error': temporal_df['error'].quantile(0.9)\n",
                "}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualization: Predicted vs Actual\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Scatter plot\n",
                "axes[0].scatter(temporal_df['actual'], temporal_df['predicted'], s=100, c='steelblue', edgecolors='black')\n",
                "max_val = max(temporal_df['actual'].max(), temporal_df['predicted'].max())\n",
                "axes[0].plot([0, max_val], [0, max_val], 'r--', linewidth=2, label='Perfect Prediction')\n",
                "axes[0].set_xlabel('Actual Funding Ratio')\n",
                "axes[0].set_ylabel('Predicted Funding Ratio')\n",
                "axes[0].set_title('Temporal Validation: Predicted vs Actual')\n",
                "axes[0].legend()\n",
                "\n",
                "# Error distribution\n",
                "axes[1].bar(range(len(temporal_df)), temporal_df['error'], color='coral')\n",
                "axes[1].axhline(y=temporal_df['error'].mean(), color='red', linestyle='--', label=f'Mean: {temporal_df[\"error\"].mean():.3f}')\n",
                "axes[1].set_xlabel('Campaign')\n",
                "axes[1].set_ylabel('Absolute Error')\n",
                "axes[1].set_title('Prediction Errors')\n",
                "axes[1].legend()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Validation 2: Placebo Test\n",
                "\n",
                "For campaigns that did NOT change strategy, the model should predict NO CHANGE."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def placebo_test(campaign, model, scaler, feature_cols):\n",
                "    \"\"\"\n",
                "    Test if model incorrectly detects phantom changes.\n",
                "    \n",
                "    For a campaign that didn't change, we simulate predicting\n",
                "    with a slightly different (fake) price. The effect should be small.\n",
                "    \"\"\"\n",
                "    # Original features\n",
                "    original_features = campaign[feature_cols].values.reshape(1, -1)\n",
                "    \n",
                "    # Placebo: add 1% noise to price (simulates no real change)\n",
                "    placebo_features = original_features.copy()\n",
                "    price_idx = feature_cols.index('avg_reward_price')\n",
                "    noise = np.random.normal(0, 0.01) * original_features[0, price_idx]\n",
                "    placebo_features[0, price_idx] += noise\n",
                "    \n",
                "    # Scale and predict\n",
                "    original_scaled = scaler.transform(original_features)\n",
                "    placebo_scaled = scaler.transform(placebo_features)\n",
                "    \n",
                "    original_pred = model.predict(original_scaled)[0]\n",
                "    placebo_pred = model.predict(placebo_scaled)[0]\n",
                "    \n",
                "    phantom_effect = abs(placebo_pred - original_pred)\n",
                "    return phantom_effect"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Select 50 random campaigns with LOW price variation (didn't experiment)\n",
                "no_change_campaigns = df.nsmallest(100, 'price_variation').sample(50, random_state=42)\n",
                "\n",
                "np.random.seed(42)\n",
                "placebo_effects = []\n",
                "for idx, row in no_change_campaigns.iterrows():\n",
                "    effect = placebo_test(row, ols_model, scaler, feature_cols)\n",
                "    placebo_effects.append(effect)\n",
                "\n",
                "print(\"\\nPLACEBO TEST RESULTS\")\n",
                "print(\"=\"*50)\n",
                "print(f\"Mean phantom effect: {np.mean(placebo_effects):.6f}\")\n",
                "print(f\"Median phantom effect: {np.median(placebo_effects):.6f}\")\n",
                "print(f\"95th percentile: {np.percentile(placebo_effects, 95):.6f}\")\n",
                "print(f\"Max phantom effect: {np.max(placebo_effects):.6f}\")\n",
                "\n",
                "# Check if placebo effects are small\n",
                "threshold = 0.01\n",
                "passed = np.mean(placebo_effects) < threshold\n",
                "print(f\"\\nTest {'PASSED' if passed else 'FAILED'}: Mean phantom effect {'<' if passed else '>='} {threshold}\")\n",
                "\n",
                "validation_results['placebo'] = {\n",
                "    'mean_phantom_effect': np.mean(placebo_effects),\n",
                "    'p95_phantom_effect': np.percentile(placebo_effects, 95),\n",
                "    'passed': passed\n",
                "}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize placebo effects\n",
                "fig, ax = plt.subplots(figsize=(10, 5))\n",
                "\n",
                "ax.hist(placebo_effects, bins=30, color='teal', edgecolor='black', alpha=0.7)\n",
                "ax.axvline(x=np.mean(placebo_effects), color='red', linestyle='--', \n",
                "           linewidth=2, label=f'Mean: {np.mean(placebo_effects):.6f}')\n",
                "ax.axvline(x=np.percentile(placebo_effects, 95), color='orange', linestyle='--',\n",
                "           linewidth=2, label=f'95th pct: {np.percentile(placebo_effects, 95):.6f}')\n",
                "\n",
                "ax.set_xlabel('Phantom Effect (should be ~0)')\n",
                "ax.set_ylabel('Count')\n",
                "ax.set_title('Placebo Test: Distribution of Phantom Effects')\n",
                "ax.legend()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Validation 3: Manski Bounds\n",
                "\n",
                "Check if point estimates fall within theoretically plausible bounds."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def manski_bounds(observed_outcome, treatment_effect, uncertainty=0.3):\n",
                "    \"\"\"\n",
                "    Compute worst-case and best-case bounds assuming unobserved confounding.\n",
                "    \n",
                "    Args:\n",
                "        observed_outcome: Actual funding ratio\n",
                "        treatment_effect: Estimated treatment effect\n",
                "        uncertainty: Range of unobserved confounding\n",
                "        \n",
                "    Returns:\n",
                "        (lower_bound, upper_bound)\n",
                "    \"\"\"\n",
                "    lower_bound = observed_outcome + treatment_effect - uncertainty\n",
                "    upper_bound = observed_outcome + treatment_effect + uncertainty\n",
                "    return lower_bound, upper_bound"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Sample 100 campaigns for bounds analysis\n",
                "sample_df = df.sample(100, random_state=42).copy()\n",
                "\n",
                "# Get predictions\n",
                "sample_features = sample_df[feature_cols].values\n",
                "sample_scaled = scaler.transform(sample_features)\n",
                "sample_df['predicted'] = ols_model.predict(sample_scaled)\n",
                "\n",
                "# Compute treatment effects (difference between predicted and counterfactual)\n",
                "tsls_coef = models.get('tsls_coef', 0.00002)  # Use 2SLS coefficient\n",
                "sample_df['treatment_effect'] = tsls_coef * sample_df['avg_reward_price']\n",
                "\n",
                "# Compute bounds\n",
                "bounds_results = []\n",
                "within_bounds = 0\n",
                "\n",
                "for idx, row in sample_df.iterrows():\n",
                "    lb, ub = manski_bounds(row['funding_ratio'], row['treatment_effect'], uncertainty=0.3)\n",
                "    point_est = row['predicted']\n",
                "    \n",
                "    is_within = lb <= point_est <= ub\n",
                "    if is_within:\n",
                "        within_bounds += 1\n",
                "    \n",
                "    bounds_results.append({\n",
                "        'lower_bound': lb,\n",
                "        'upper_bound': ub,\n",
                "        'point_estimate': point_est,\n",
                "        'within_bounds': is_within\n",
                "    })\n",
                "\n",
                "bounds_df = pd.DataFrame(bounds_results)\n",
                "pct_within = within_bounds / len(sample_df) * 100\n",
                "\n",
                "print(\"\\nMANSKI BOUNDS RESULTS\")\n",
                "print(\"=\"*50)\n",
                "print(f\"Campaigns within bounds: {within_bounds}/{len(sample_df)} ({pct_within:.1f}%)\")\n",
                "print(f\"Average bound width: {(bounds_df['upper_bound'] - bounds_df['lower_bound']).mean():.3f}\")\n",
                "\n",
                "validation_results['manski'] = {\n",
                "    'pct_within_bounds': pct_within,\n",
                "    'avg_bound_width': (bounds_df['upper_bound'] - bounds_df['lower_bound']).mean()\n",
                "}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize bounds\n",
                "fig, ax = plt.subplots(figsize=(12, 6))\n",
                "\n",
                "sample_idx = range(20)  # Show first 20\n",
                "bounds_sample = bounds_df.iloc[:20]\n",
                "\n",
                "# Plot bounds\n",
                "ax.vlines(sample_idx, bounds_sample['lower_bound'], bounds_sample['upper_bound'], \n",
                "          color='blue', linewidth=3, alpha=0.5, label='Manski Bounds')\n",
                "ax.scatter(sample_idx, bounds_sample['point_estimate'], color='red', s=80, zorder=5, label='Point Estimate')\n",
                "\n",
                "# Color by within/outside bounds\n",
                "for i, (idx, row) in enumerate(bounds_sample.iterrows()):\n",
                "    if not row['within_bounds']:\n",
                "        ax.scatter([i], [row['point_estimate']], color='orange', s=100, edgecolors='black', zorder=6)\n",
                "\n",
                "ax.set_xlabel('Campaign')\n",
                "ax.set_ylabel('Funding Ratio')\n",
                "ax.set_title('Manski Bounds: Point Estimates vs Theoretical Bounds')\n",
                "ax.legend()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Validation 4: Temporal Cross-Validation\n",
                "\n",
                "Train on 2020-2022, test on 2023-2024 to verify causal stability."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Split by year\n",
                "df['year'] = df['launch_date'].dt.year\n",
                "\n",
                "train_df = df[df['year'] <= 2022].dropna(subset=feature_cols + ['funding_ratio'])\n",
                "test_df = df[df['year'] >= 2023].dropna(subset=feature_cols + ['funding_ratio'])\n",
                "\n",
                "print(f\"Training set (2020-2022): {len(train_df)} campaigns\")\n",
                "print(f\"Test set (2023-2024): {len(test_df)} campaigns\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train on historical data\n",
                "X_train_temporal = train_df[feature_cols]\n",
                "y_train_temporal = train_df['funding_ratio']\n",
                "\n",
                "X_test_temporal = test_df[feature_cols]\n",
                "y_test_temporal = test_df['funding_ratio']\n",
                "\n",
                "# Scale\n",
                "scaler_temporal = StandardScaler()\n",
                "X_train_scaled = scaler_temporal.fit_transform(X_train_temporal)\n",
                "X_test_scaled = scaler_temporal.transform(X_test_temporal)\n",
                "\n",
                "# Train model\n",
                "model_temporal = LinearRegression().fit(X_train_scaled, y_train_temporal)\n",
                "\n",
                "# Evaluate\n",
                "train_pred = model_temporal.predict(X_train_scaled)\n",
                "test_pred = model_temporal.predict(X_test_scaled)\n",
                "\n",
                "train_r2 = r2_score(y_train_temporal, train_pred)\n",
                "test_r2 = r2_score(y_test_temporal, test_pred) if len(test_df) > 0 else np.nan\n",
                "test_mae = mean_absolute_error(y_test_temporal, test_pred) if len(test_df) > 0 else np.nan\n",
                "\n",
                "# Compare coefficients\n",
                "train_price_coef = model_temporal.coef_[0]\n",
                "full_price_coef = ols_model.coef_[0]\n",
                "coef_change = abs(train_price_coef - full_price_coef)\n",
                "\n",
                "print(\"\\nTEMPORAL CROSS-VALIDATION RESULTS\")\n",
                "print(\"=\"*50)\n",
                "print(f\"Training R² (2020-2022): {train_r2:.4f}\")\n",
                "print(f\"Test R² (2023-2024): {test_r2:.4f}\" if not np.isnan(test_r2) else \"Test R²: N/A (no test data)\")\n",
                "print(f\"Test MAE: {test_mae:.4f}\" if not np.isnan(test_mae) else \"Test MAE: N/A\")\n",
                "print(f\"\\nPrice coefficient (2020-2022 trained): {train_price_coef:.6f}\")\n",
                "print(f\"Price coefficient (full data): {full_price_coef:.6f}\")\n",
                "print(f\"Coefficient change: {coef_change:.6f}\")\n",
                "\n",
                "coef_stable = coef_change < 0.01\n",
                "print(f\"\\nTreatment effect stable: {'YES' if coef_stable else 'NO'} (change {'<' if coef_stable else '>='} 0.01)\")\n",
                "\n",
                "validation_results['temporal_cv'] = {\n",
                "    'train_r2': train_r2,\n",
                "    'test_r2': test_r2 if not np.isnan(test_r2) else 'N/A',\n",
                "    'test_mae': test_mae if not np.isnan(test_mae) else 'N/A',\n",
                "    'coef_change': coef_change,\n",
                "    'stable': coef_stable\n",
                "}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Validation Report"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "report = \"\"\"\n",
                "================================================================================\n",
                "                        VALIDATION RESULTS REPORT\n",
                "================================================================================\n",
                "\n",
                "### Temporal Validation (N={temporal_n} campaigns)\n",
                "- Mean Absolute Error: {temporal_mae:.4f}\n",
                "- Median Absolute Error: {temporal_median:.4f}\n",
                "- 90% of predictions within ±{temporal_p90:.4f} of actual\n",
                "\n",
                "### Placebo Test (N=50 campaigns)\n",
                "- Mean phantom effect: {placebo_mean:.6f} (should be ~0)\n",
                "- 95% of effects < {placebo_p95:.6f} (should be small)\n",
                "- Test: {placebo_status}\n",
                "\n",
                "### Manski Bounds Check (N=100 campaigns)\n",
                "- {manski_pct:.1f}% of point estimates fall within theoretical bounds\n",
                "- Average bound width: {manski_width:.3f}\n",
                "\n",
                "### Temporal Cross-Validation\n",
                "- 2020-2022 training → 2023-2024 test: R² = {cv_r2}\n",
                "- Treatment effect stable: {cv_stable} (ΔCoef = {cv_change:.6f})\n",
                "\n",
                "### Conclusion\n",
                "Model is {conclusion} for counterfactual prediction because:\n",
                "- Placebo test {placebo_reason}\n",
                "- {manski_pct:.0f}% of estimates are within bounds\n",
                "- Treatment effects are {stability_reason} over time\n",
                "\n",
                "================================================================================\n",
                "\"\"\".format(\n",
                "    temporal_n=len(temporal_df),\n",
                "    temporal_mae=validation_results['temporal']['mae'],\n",
                "    temporal_median=validation_results['temporal']['median_ae'],\n",
                "    temporal_p90=validation_results['temporal']['p90_error'],\n",
                "    placebo_mean=validation_results['placebo']['mean_phantom_effect'],\n",
                "    placebo_p95=validation_results['placebo']['p95_phantom_effect'],\n",
                "    placebo_status='PASSED ✓' if validation_results['placebo']['passed'] else 'FAILED ✗',\n",
                "    manski_pct=validation_results['manski']['pct_within_bounds'],\n",
                "    manski_width=validation_results['manski']['avg_bound_width'],\n",
                "    cv_r2=validation_results['temporal_cv']['test_r2'],\n",
                "    cv_stable='YES ✓' if validation_results['temporal_cv']['stable'] else 'NO ✗',\n",
                "    cv_change=validation_results['temporal_cv']['coef_change'],\n",
                "    conclusion='RELIABLE' if validation_results['placebo']['passed'] and validation_results['temporal_cv']['stable'] else 'NEEDS IMPROVEMENT',\n",
                "    placebo_reason='passed (no phantom effects detected)' if validation_results['placebo']['passed'] else 'failed (detecting spurious effects)',\n",
                "    stability_reason='stable' if validation_results['temporal_cv']['stable'] else 'unstable'\n",
                ")\n",
                "\n",
                "print(report)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save validation metrics\n",
                "validation_df = pd.DataFrame([\n",
                "    {'metric': 'Temporal MAE', 'value': validation_results['temporal']['mae']},\n",
                "    {'metric': 'Temporal Median AE', 'value': validation_results['temporal']['median_ae']},\n",
                "    {'metric': 'Placebo Mean Effect', 'value': validation_results['placebo']['mean_phantom_effect']},\n",
                "    {'metric': 'Placebo Test Passed', 'value': int(validation_results['placebo']['passed'])},\n",
                "    {'metric': 'Manski Pct Within Bounds', 'value': validation_results['manski']['pct_within_bounds']},\n",
                "    {'metric': 'Temporal CV Test R2', 'value': validation_results['temporal_cv']['test_r2'] if validation_results['temporal_cv']['test_r2'] != 'N/A' else np.nan},\n",
                "    {'metric': 'Coefficient Stable', 'value': int(validation_results['temporal_cv']['stable'])}\n",
                "])\n",
                "\n",
                "validation_df.to_csv('../data/processed/validation_results.csv', index=False)\n",
                "print(\"Validation results saved to data/processed/validation_results.csv\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Summary\n",
                "\n",
                "The validation suite tests our causal models across four dimensions:\n",
                "\n",
                "1. **Temporal Validation**: How accurately can we predict outcomes?\n",
                "2. **Placebo Test**: Does the model avoid detecting false effects?\n",
                "3. **Manski Bounds**: Are estimates within theoretical limits?\n",
                "4. **Temporal CV**: Are causal effects stable over time?\n",
                "\n",
                "→ Proceed to building the Streamlit dashboard with validated models."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}